{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0979bcc8-5767-4fb7-82c8-5fdb64b4bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b5877-a885-45d9-8f7c-206ff0d0874b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b141004e-acf4-4767-b8f5-f0d996ae6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ViTFeatureExtractor, ViTModel, BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1438c-6862-4234-a42f-9d861b6dfd71",
   "metadata": {},
   "source": [
    "# Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98833aaf-807a-4e82-882c-b82de503d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_model_name)\n",
    "        self.vit_model = ViTModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, video_frames):\n",
    "        # video_frames: (batch_size, num_frames, channels, height, width)\n",
    "        \n",
    "        batch_size, num_frames, _, _, _ = video_frames.size()\n",
    "        \n",
    "        # Reshape video_frames to (batch_size * num_frames, channels, height, width)\n",
    "        video_frames = video_frames.view(-1, *video_frames.shape[2:])\n",
    "        \n",
    "        # Extract features using ViT\n",
    "        inputs = self.feature_extractor(images=video_frames, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(video_frames.device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.vit_model(**inputs)\n",
    "        \n",
    "        # Extract the features from the model's output\n",
    "        features = outputs.last_hidden_state  # (batch_size * num_frames, seq_len, hidden_size)\n",
    "        \n",
    "        # Reshape features to (batch_size, num_frames, seq_len, hidden_size)\n",
    "        features = features.view(batch_size, num_frames, *features.shape[1:])\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e220d4a8-23dc-4f66-ad5b-ded5e85030d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        # captions: (batch_size, seq_len)\n",
    "        \n",
    "        # Tokenize captions and get BERT embeddings\n",
    "        input_ids = captions\n",
    "        attention_mask = (input_ids != 0).float()  # Create attention mask (0 indicates padding)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the BERT embeddings from the model's output\n",
    "        embeddings = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a10486-f530-4ae6-a2fe-415d5bb28039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioningModel(nn.Module):\n",
    "    def __init__(self, video_encoder, text_encoder):\n",
    "        super(VideoCaptioningModel, self).__init__()\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, video_features, captions):\n",
    "        video_encoded = self.video_encoder(video_features)\n",
    "        text_encoded = self.text_encoder(captions)\n",
    "        similarity = similarity_loss(video_encoded, text_encoded)\n",
    "        return similarity\n",
    "\n",
    "model = VideoCaptioningModel(video_encoder, text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6897ec84-7dc5-4cba-9dbd-9d047f916c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_loss(video_encoded, text_encoded):\n",
    "    # Normalize the encodings\n",
    "    video_encoded = F.normalize(video_encoded, p=2, dim=-1)\n",
    "    text_encoded = F.normalize(text_encoded, p=2, dim=-1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = torch.matmul(video_encoded, text_encoded.transpose(1, 2))\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    # In this case, you want to maximize similarity, so use -log(probability) as the loss\n",
    "    loss = -torch.log(similarity + 1e-8)  # Adding a small epsilon to avoid log(0)\n",
    "\n",
    "    # Calculate the mean loss over the batch\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3265a9d0-18a4-467c-bdae-85293c109fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for video-caption pairs\n",
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, video_folder, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.transform = transform\n",
    "        self.data = self.load_json_data(json_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_info = self.data[idx]\n",
    "        video_id = video_info['video_id']\n",
    "        video_path = os.path.join(slef.video_folder, f'{video_id}.mp4')\n",
    "        captions = video_info['captions']\n",
    "\n",
    "        # Load video frames and apply transformations\n",
    "        video_frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            video_frames = [self.transform(frame) for frame in video_frames]\n",
    "\n",
    "        return video_frames, captions\n",
    "\n",
    "    def load_json_data(self, json_path):\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(videopath)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert frame to PIL image\n",
    "            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.BGR2RGB))\n",
    "            frames.append(frame_pil)\n",
    "\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56e85a-b245-4aa6-afc9-e564d6b63906",
   "metadata": {},
   "source": [
    "# Setup video encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a797e45b-ee54-4c0a-b83e-d56104fd59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for video frames (you can customize these)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define paths and create data loaders for training and validation\n",
    "json_path = 'train_val_annotation/train_val_videodatainfo.json'  # Path to your JSON file\n",
    "video_folder = 'TrainValVideo'  # Path to the folder containing video files\n",
    "\n",
    "dataset = VideoCaptionDataset(json_path, video_folder, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae900e05-6e37-4eda-9a1f-a072acd88112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijoysarkar/video-captioning/venv/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "video_encoder = VideoEncoder(pretrained_model_name=\"google/vit-base-patch16-224-in21k\", hidden_size=768)\n",
    "text_encoder = TextEncoder(pretrained_model_name=\"bert-base-uncased\", hidden_size=768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff8dc7-1e34-4bc6-b895-9d66c9d9d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc526e-0ea9-491a-bb8b-c3bc970b1ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807bf471-d137-413c-a08b-2f1a8f9b04fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98390b35-ddaa-4e7d-abbb-811068274fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoCaptioningModel(video_encoder, text_encoder)\n",
    "criterion = nn.MSELoss()  # You can use any suitable loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee2711-9cd5-4e2b-a53d-c7d3ef689ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
