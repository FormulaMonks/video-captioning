{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0979bcc8-5767-4fb7-82c8-5fdb64b4bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b5877-a885-45d9-8f7c-206ff0d0874b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b141004e-acf4-4767-b8f5-f0d996ae6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ViTFeatureExtractor, ViTModel, BertTokenizer, BertModel\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1438c-6862-4234-a42f-9d861b6dfd71",
   "metadata": {},
   "source": [
    "# Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98833aaf-807a-4e82-882c-b82de503d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_model_name)\n",
    "        self.vit_model = ViTModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, video_frames):\n",
    "        # video_frames: (batch_size, num_frames, channels, height, width)\n",
    "        \n",
    "        batch_size, num_frames, _, _, _ = video_frames.size()\n",
    "        \n",
    "        # Reshape video_frames to (batch_size * num_frames, channels, height, width)\n",
    "        video_frames = video_frames.view(-1, *video_frames.shape[2:])\n",
    "        \n",
    "        # Extract features using ViT\n",
    "        inputs = self.feature_extractor(images=video_frames, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(video_frames.device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.vit_model(**inputs)\n",
    "        \n",
    "        # Extract the features from the model's output\n",
    "        features = outputs.last_hidden_state  # (batch_size * num_frames, seq_len, hidden_size)\n",
    "        \n",
    "        # Reshape features to (batch_size, num_frames, seq_len, hidden_size)\n",
    "        features = features.view(batch_size, num_frames, *features.shape[1:])\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e220d4a8-23dc-4f66-ad5b-ded5e85030d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        # captions: (batch_size, seq_len)\n",
    "        \n",
    "        # Tokenize captions and get BERT embeddings\n",
    "        input_ids = captions\n",
    "        attention_mask = (input_ids != 0).float()  # Create attention mask (0 indicates padding)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the BERT embeddings from the model's output\n",
    "        embeddings = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6897ec84-7dc5-4cba-9dbd-9d047f916c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_loss(video_encoded, text_encoded):\n",
    "    # Normalize the encodings\n",
    "    video_encoded = F.normalize(video_encoded, p=2, dim=-1)\n",
    "    text_encoded = F.normalize(text_encoded, p=2, dim=-1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = torch.matmul(video_encoded, text_encoded.transpose(1, 2))\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    # In this case, you want to maximize similarity, so use -log(probability) as the loss\n",
    "    loss = -torch.log(similarity + 1e-8)  # Adding a small epsilon to avoid log(0)\n",
    "\n",
    "    # Calculate the mean loss over the batch\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19a10486-f530-4ae6-a2fe-415d5bb28039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptioningModel(nn.Module):\n",
    "    def __init__(self, video_encoder, text_encoder):\n",
    "        super(VideoCaptioningModel, self).__init__()\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, video_features, captions):\n",
    "        video_encoded = self.video_encoder(video_features)\n",
    "        text_encoded = self.text_encoder(captions)\n",
    "        similarity = similarity_loss(video_encoded, text_encoded)\n",
    "        return similarity\n",
    "\n",
    "model = VideoCaptioningModel(video_encoder, text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3265a9d0-18a4-467c-bdae-85293c109fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for video-caption pairs\n",
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, json_path, video_folder, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.transform = transform\n",
    "        self.data = pd.DataFrame(self.load_json_data(json_path)[\"sentences\"])\n",
    "        self.data.set_index('video_id', inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index.unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx = f'video{idx}'\n",
    "        video_path = os.path.join(self.video_folder, f'{video_idx}.mp4')\n",
    "        captions = self.data.loc[video_idx][\"caption\"].tolist()\n",
    "\n",
    "        # Load video frames and apply transformations\n",
    "        video_frames = self.load_video_frames(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            video_frames = [self.transform(frame) for frame in video_frames]\n",
    "\n",
    "        return video_frames, captions\n",
    "\n",
    "    def load_json_data(self, json_path):\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(videopath)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert frame to PIL image\n",
    "            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.BGR2RGB))\n",
    "            frames.append(frame_pil)\n",
    "\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56e85a-b245-4aa6-afc9-e564d6b63906",
   "metadata": {},
   "source": [
    "# Setup video encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a797e45b-ee54-4c0a-b83e-d56104fd59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for video frames (you can customize these)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define paths and create data loaders for training and validation\n",
    "json_path = 'train_val_annotation/train_val_videodatainfo.json'  # Path to your JSON file\n",
    "video_folder = 'TrainValVideo'  # Path to the folder containing video files\n",
    "\n",
    "dataset = VideoCaptionDataset(json_path, video_folder, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae900e05-6e37-4eda-9a1f-a072acd88112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijoysarkar/video-captioning/venv/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "video_encoder = VideoEncoder(pretrained_model_name=\"google/vit-base-patch16-224-in21k\", hidden_size=768)\n",
    "text_encoder = TextEncoder(pretrained_model_name=\"bert-base-uncased\", hidden_size=768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98390b35-ddaa-4e7d-abbb-811068274fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoCaptioningModel(video_encoder, text_encoder)\n",
    "criterion = nn.MSELoss()  # You can use any suitable loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10ee2711-9cd5-4e2b-a53d-c7d3ef689ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'video69796'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:219\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:227\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:119\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'video69796'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     video_features, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 15\u001b[0m, in \u001b[0;36mVideoCaptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m video_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m captions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load video frames and apply transformations\u001b[39;00m\n\u001b[1;32m     18\u001b[0m video_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_video_frames(video_path)\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1393\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1343\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/generic.py:4236\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4234\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4236\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/video-captioning/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'video69796'"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        video_features, captions = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        similarity = model(video_features, captions)\n",
    "\n",
    "        # Backpropagation\n",
    "        similarity.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += similarity.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8245c245-97fb-4418-8114-1527cdb01b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"train_val_annotation/train_val_videodatainfo.json\"\n",
    "with open(json_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d02fdef0-b8e6-4a44-aebb-1638e1638177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'caption': 'a cartoon animals runs through an ice cave in a video game',\n",
       "  'video_id': 'video2960',\n",
       "  'sen_id': 0},\n",
       " {'caption': 'a cartoon character runs around inside of a video game',\n",
       "  'video_id': 'video2960',\n",
       "  'sen_id': 1},\n",
       " {'caption': 'a character is running in the snow',\n",
       "  'video_id': 'video2960',\n",
       "  'sen_id': 2},\n",
       " {'caption': 'a person plays a video game centered around ice age the movie',\n",
       "  'video_id': 'video2960',\n",
       "  'sen_id': 3},\n",
       " {'caption': 'a person plays online and records themselves',\n",
       "  'video_id': 'video2960',\n",
       "  'sen_id': 4}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentences\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00d54acf-4057-4eb1-8ec4-5389a35fbc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>video_id</th>\n",
       "      <th>sen_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a cartoon animals runs through an ice cave in ...</td>\n",
       "      <td>video2960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a cartoon character runs around inside of a vi...</td>\n",
       "      <td>video2960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a character is running in the snow</td>\n",
       "      <td>video2960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a person plays a video game centered around ic...</td>\n",
       "      <td>video2960</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a person plays online and records themselves</td>\n",
       "      <td>video2960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140195</th>\n",
       "      <td>two soldiers speak to a camera outside</td>\n",
       "      <td>video140</td>\n",
       "      <td>140195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140196</th>\n",
       "      <td>two soldiers talking about another soldier</td>\n",
       "      <td>video140</td>\n",
       "      <td>140196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140197</th>\n",
       "      <td>two soldiers talking on duty</td>\n",
       "      <td>video140</td>\n",
       "      <td>140197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140198</th>\n",
       "      <td>two troops speak with one another</td>\n",
       "      <td>video140</td>\n",
       "      <td>140198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140199</th>\n",
       "      <td>soldiers in uniform talking together</td>\n",
       "      <td>video140</td>\n",
       "      <td>140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  caption   video_id  sen_id\n",
       "0       a cartoon animals runs through an ice cave in ...  video2960       0\n",
       "1       a cartoon character runs around inside of a vi...  video2960       1\n",
       "2                      a character is running in the snow  video2960       2\n",
       "3       a person plays a video game centered around ic...  video2960       3\n",
       "4            a person plays online and records themselves  video2960       4\n",
       "...                                                   ...        ...     ...\n",
       "140195             two soldiers speak to a camera outside   video140  140195\n",
       "140196         two soldiers talking about another soldier   video140  140196\n",
       "140197                       two soldiers talking on duty   video140  140197\n",
       "140198                  two troops speak with one another   video140  140198\n",
       "140199               soldiers in uniform talking together   video140  140199\n",
       "\n",
       "[140200 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data[\"sentences\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b2682ca-843f-47ea-b48f-b3c1dbdaf227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>sen_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>video2960</th>\n",
       "      <td>a cartoon animals runs through an ice cave in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video2960</th>\n",
       "      <td>a cartoon character runs around inside of a vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video2960</th>\n",
       "      <td>a character is running in the snow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video2960</th>\n",
       "      <td>a person plays a video game centered around ic...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video2960</th>\n",
       "      <td>a person plays online and records themselves</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video140</th>\n",
       "      <td>two soldiers speak to a camera outside</td>\n",
       "      <td>140195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video140</th>\n",
       "      <td>two soldiers talking about another soldier</td>\n",
       "      <td>140196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video140</th>\n",
       "      <td>two soldiers talking on duty</td>\n",
       "      <td>140197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video140</th>\n",
       "      <td>two troops speak with one another</td>\n",
       "      <td>140198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video140</th>\n",
       "      <td>soldiers in uniform talking together</td>\n",
       "      <td>140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     caption  sen_id\n",
       "video_id                                                            \n",
       "video2960  a cartoon animals runs through an ice cave in ...       0\n",
       "video2960  a cartoon character runs around inside of a vi...       1\n",
       "video2960                 a character is running in the snow       2\n",
       "video2960  a person plays a video game centered around ic...       3\n",
       "video2960       a person plays online and records themselves       4\n",
       "...                                                      ...     ...\n",
       "video140              two soldiers speak to a camera outside  140195\n",
       "video140          two soldiers talking about another soldier  140196\n",
       "video140                        two soldiers talking on duty  140197\n",
       "video140                   two troops speak with one another  140198\n",
       "video140                soldiers in uniform talking together  140199\n",
       "\n",
       "[140200 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('video_id', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c86b8f1-1362-4ff0-b169-294a87631889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7010"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8455ba56-994c-45f2-b2d6-db3764b263ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>video_id</th>\n",
       "      <th>start time</th>\n",
       "      <th>end time</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>https://www.youtube.com/watch?v=9lZi22qLlEo</td>\n",
       "      <td>video0</td>\n",
       "      <td>137.72</td>\n",
       "      <td>149.44</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>https://www.youtube.com/watch?v=w4JM08PDEng</td>\n",
       "      <td>video1</td>\n",
       "      <td>184.33</td>\n",
       "      <td>206.89</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>https://www.youtube.com/watch?v=QA7KVQq9vKA</td>\n",
       "      <td>video2</td>\n",
       "      <td>31.17</td>\n",
       "      <td>41.24</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>https://www.youtube.com/watch?v=QFmJZ0GU6yc</td>\n",
       "      <td>video3</td>\n",
       "      <td>48.26</td>\n",
       "      <td>58.51</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>https://www.youtube.com/watch?v=2q-dONPhzis</td>\n",
       "      <td>video4</td>\n",
       "      <td>268.58</td>\n",
       "      <td>278.83</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.youtube.com/watch?v=cvzoU0yy73s</td>\n",
       "      <td>video7005</td>\n",
       "      <td>1028.68</td>\n",
       "      <td>1039.29</td>\n",
       "      <td>validate</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>7</td>\n",
       "      <td>https://www.youtube.com/watch?v=8vIzjd9ceDY</td>\n",
       "      <td>video7006</td>\n",
       "      <td>300.40</td>\n",
       "      <td>311.96</td>\n",
       "      <td>validate</td>\n",
       "      <td>7006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>14</td>\n",
       "      <td>https://www.youtube.com/watch?v=1Ov5f-B_pqg</td>\n",
       "      <td>video7007</td>\n",
       "      <td>1086.57</td>\n",
       "      <td>1098.48</td>\n",
       "      <td>validate</td>\n",
       "      <td>7007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>11</td>\n",
       "      <td>https://www.youtube.com/watch?v=wcZNNvMM9Jk</td>\n",
       "      <td>video7008</td>\n",
       "      <td>88.44</td>\n",
       "      <td>104.72</td>\n",
       "      <td>validate</td>\n",
       "      <td>7008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>19</td>\n",
       "      <td>https://www.youtube.com/watch?v=2mTLO2F_ERY</td>\n",
       "      <td>video7009</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.45</td>\n",
       "      <td>validate</td>\n",
       "      <td>7009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7010 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                          url   video_id  \\\n",
       "0            9  https://www.youtube.com/watch?v=9lZi22qLlEo     video0   \n",
       "1           16  https://www.youtube.com/watch?v=w4JM08PDEng     video1   \n",
       "2            9  https://www.youtube.com/watch?v=QA7KVQq9vKA     video2   \n",
       "3            8  https://www.youtube.com/watch?v=QFmJZ0GU6yc     video3   \n",
       "4           14  https://www.youtube.com/watch?v=2q-dONPhzis     video4   \n",
       "...        ...                                          ...        ...   \n",
       "7005         1  https://www.youtube.com/watch?v=cvzoU0yy73s  video7005   \n",
       "7006         7  https://www.youtube.com/watch?v=8vIzjd9ceDY  video7006   \n",
       "7007        14  https://www.youtube.com/watch?v=1Ov5f-B_pqg  video7007   \n",
       "7008        11  https://www.youtube.com/watch?v=wcZNNvMM9Jk  video7008   \n",
       "7009        19  https://www.youtube.com/watch?v=2mTLO2F_ERY  video7009   \n",
       "\n",
       "      start time  end time     split    id  \n",
       "0         137.72    149.44     train     0  \n",
       "1         184.33    206.89     train     1  \n",
       "2          31.17     41.24     train     2  \n",
       "3          48.26     58.51     train     3  \n",
       "4         268.58    278.83     train     4  \n",
       "...          ...       ...       ...   ...  \n",
       "7005     1028.68   1039.29  validate  7005  \n",
       "7006      300.40    311.96  validate  7006  \n",
       "7007     1086.57   1098.48  validate  7007  \n",
       "7008       88.44    104.72  validate  7008  \n",
       "7009        0.00     12.45  validate  7009  \n",
       "\n",
       "[7010 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data[\"videos\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea0ac3df-3ff7-43d6-b4bf-fada49e230f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5f0efcdb50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42b67a-90a5-4bab-a923-ce62e2371eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
